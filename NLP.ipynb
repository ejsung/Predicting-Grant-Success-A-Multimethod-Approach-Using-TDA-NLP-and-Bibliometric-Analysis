{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxGsl9B7yitK"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from datasets import Dataset\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "model_name = \"allenai/scibert_scivocab_uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "def preprocess_data(examples):\n",
        "    text = [t + \" \" + k + \" \" + a for t, k, a in zip(examples[\"title\"], examples[\"keywords\"], examples[\"author\"])]\n",
        "    return tokenizer(text, padding=\"max_length\", truncation=True, max_length=10)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "df = pd.read_excel(\"data_file.xlsx\")\n",
        "train_df, test_df = train_test_split(df, test_size=0.25, random_state=21)\n",
        "\n",
        "required_columns = {\"title\", \"keywords\", \"author\", \"label\"}\n",
        "if not required_columns.issubset(train_df.columns):\n",
        "    raise ValueError(f\"Excel file must contain {required_columns} columns.\")\n",
        "\n",
        "train_df['keywords'].fillna('No Keywords', inplace=True)\n",
        "\n",
        "dataset = Dataset.from_pandas(train_df)\n",
        "\n",
        "dataset = dataset.map(preprocess_data, batched=True)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset=dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "if not {\"title\", \"keywords\", \"author\"}.issubset(test_df.columns):\n",
        "    raise ValueError(\"Excel file must contain 'title', 'keywords', and 'author' columns.\")\n",
        "\n",
        "test_df['keywords'].fillna('No Keywords', inplace=True)\n",
        "\n",
        "test_titles = test_df[\"title\"].tolist()\n",
        "test_keywords = test_df[\"keywords\"].tolist()\n",
        "test_authors = test_df[\"author\"].tolist()\n",
        "\n",
        "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "inputs = tokenizer(\n",
        "    [t + \" \" + k + \" \" + a for t, k, a in zip(test_titles, test_keywords, test_authors)],\n",
        "    padding=True, truncation=True, return_tensors=\"pt\", max_length=10\n",
        ")\n",
        "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "outputs = model(**inputs)\n",
        "predictions = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "test_df[\"Prediction\"] = predictions.cpu().numpy()\n",
        "\n",
        "test_df.to_excel(\"output_file.xlsx\", index=False)"
      ]
    }
  ]
}